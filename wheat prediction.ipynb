{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f342fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d8361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f9981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 629 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\NIDHI\\Desktop\\train\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107102dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\NIDHI\\Desktop\\agriculture time series prediction\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1edbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIDHI\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size[0], img_size[1], 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # 4 classes: tillering, stem extension, heading, ripening\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7b0803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIDHI\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 6s/step - accuracy: 0.3174 - loss: 3.1119 - val_accuracy: 0.3750 - val_loss: 1.2432\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/19\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 577ms/step - accuracy: 0.3125 - loss: 1.2888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIDHI\\anaconda3\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.3125 - loss: 1.2888 - val_accuracy: 0.3000 - val_loss: 1.3793\n",
      "Epoch 3/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 5s/step - accuracy: 0.5218 - loss: 1.1475 - val_accuracy: 0.6275 - val_loss: 0.8857\n",
      "Epoch 4/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4375 - loss: 1.1796 - val_accuracy: 0.6500 - val_loss: 0.8529\n",
      "Epoch 5/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 5s/step - accuracy: 0.6272 - loss: 0.8256 - val_accuracy: 0.6875 - val_loss: 0.7510\n",
      "Epoch 6/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 0.6875 - loss: 0.7152 - val_accuracy: 0.6975 - val_loss: 0.7036\n",
      "Epoch 7/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 5s/step - accuracy: 0.6863 - loss: 0.7357 - val_accuracy: 0.7175 - val_loss: 0.5851\n",
      "Epoch 8/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.6250 - loss: 0.6721 - val_accuracy: 0.7525 - val_loss: 0.5866\n",
      "Epoch 9/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - accuracy: 0.7420 - loss: 0.5892 - val_accuracy: 0.7425 - val_loss: 0.5059\n",
      "Epoch 10/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.5396 - val_accuracy: 0.7150 - val_loss: 0.5323\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8fe88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for filename in os.listdir(r\"C:\\Users\\NIDHI\\Desktop\\test\"):\n",
    "    \n",
    "    img = load_img(os.path.join(r\"C:\\Users\\NIDHI\\Desktop\\test\", filename), target_size=img_size)\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    \n",
    "    test_images.append(img_array)\n",
    "    \n",
    "    \n",
    "    test_labels.append(0)\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow(test_images, test_labels, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1719ba97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Test accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "accuracy = np.mean(predicted_labels == predicted_labels)\n",
    "print(f'Test accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c3b0cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "Image: IMG20230103155201.jpg, Predicted Stage: Ripening\n",
      "Image: IMG20230103155205.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230105_115656.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230105_115711.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230105_115759.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230105_115816.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230110_121727.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230110_121752.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230110_121812.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230110_121824.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230117_120458.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230117_120512.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230117_120525.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230117_120538.jpg, Predicted Stage: Stem Extension\n",
      "Image: IMG_20230215_105646.jpg, Predicted Stage: Tillering\n",
      "Image: IMG_20230215_105656.jpg, Predicted Stage: Tillering\n",
      "Image: IMG_20230215_105707.jpg, Predicted Stage: Tillering\n",
      "Image: IMG_20230215_105720.jpg, Predicted Stage: Tillering\n",
      "Image: IMG_20230221_124837.jpg, Predicted Stage: Tillering\n",
      "Image: IMG_20230221_124843.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230221_124846.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230221_124855.jpg, Predicted Stage: Ripening\n",
      "Image: IMG_20230227_120246.jpg, Predicted Stage: Stem Extension\n",
      "Image: IMG_20230227_120259.jpg, Predicted Stage: Stem Extension\n",
      "Image: IMG_20230227_120310.jpg, Predicted Stage: Stem Extension\n",
      "Image: IMG_20230227_120321.jpg, Predicted Stage: Stem Extension\n"
     ]
    }
   ],
   "source": [
    "# Define the class names\n",
    "class_names = ['Tillering', 'Stem Extension', 'Heading', 'Ripening']\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Print the predictions\n",
    "for i, filename in enumerate(os.listdir(r\"C:\\Users\\NIDHI\\Desktop\\test\")):\n",
    "    print(f\"Image: {filename}, Predicted Stage: {class_names[predicted_labels[i]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c92e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
